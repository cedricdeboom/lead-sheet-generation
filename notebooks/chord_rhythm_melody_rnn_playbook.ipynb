{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f58c85ae210>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(3333)\n",
    "torch.manual_seed(3333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DATALOADER FOR MUSIC FILES\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class MusicDataset(Dataset):\n",
    "    def __init__(self, data_file, sequence_length, data_augmentation):\n",
    "        super(MusicDataset, self).__init__()\n",
    "        self.sequence_length = sequence_length\n",
    "        self.data_augmentation = data_augmentation\n",
    "        \n",
    "        with open(data_file, 'rb') as f:\n",
    "            self.id_to_sheet = pickle.load(f)\n",
    "            self.data = pickle.load(f)\n",
    "        \n",
    "        self.data = [x.astype(np.float32) for x in self.data]\n",
    "        self.start_token = np.zeros((1, 130), dtype=np.float32)\n",
    "        self.start_token[:, 0] = 1.0\n",
    "        \n",
    "        # pad all sequences to desired sequence length\n",
    "        self.mask_lengths = []\n",
    "        for i, x in enumerate(self.data):\n",
    "            if len(x) < self.sequence_length:\n",
    "                s = x.shape\n",
    "                self.data[i] = np.zeros((self.sequence_length, s[1]), dtype=np.float32)\n",
    "                self.data[i][:s[0], :] = x\n",
    "                self.mask_lengths.append(s[0])\n",
    "            else:\n",
    "                self.mask_lengths.append(self.sequence_length)\n",
    "\n",
    "    # data augmentation = shifting of chords +/- 1 octave\n",
    "    # there are 4 modes per chord, so we shift in multiples of 4\n",
    "    def augment_sequence(self, sequence, offset):\n",
    "        sequence[:, 1:128] = np.roll(sequence[:, 1:128], offset)  # melody\n",
    "        sequence[:, 143:191] = np.roll(sequence[:, 143:191], offset*4)  # chord\n",
    "        \n",
    "    def process_sequence(self, seq):\n",
    "        seq_chords_rhythm = seq[:, 130:]\n",
    "        seq_melody = np.vstack((self.start_token, seq[:, :130]))\n",
    "        \n",
    "        seq_input = np.hstack((seq_melody[:-1, :], seq_chords_rhythm))\n",
    "        seq_output = seq_melody[1:]\n",
    "        \n",
    "        return (seq_input, seq_output)\n",
    "                    \n",
    "    def __len__(self):\n",
    "        return sys.maxsize\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        data_index = np.random.randint(0, len(self.data))\n",
    "        data_point = self.data[data_index]\n",
    "        if self.data_augmentation:\n",
    "            self.augment_sequence(data_point, np.random.randint(-12, 13))\n",
    "        \n",
    "        data_range = np.random.randint(0, len(data_point) - self.sequence_length + 1)\n",
    "        seq = data_point[data_range:(data_range + self.sequence_length)]\n",
    "        \n",
    "        seq_chords_rhythm = seq[:, 130:]\n",
    "        seq_melody = np.vstack((self.start_token, seq[:, :130]))\n",
    "        \n",
    "        seq_input = np.hstack((seq_melody[:-1, :], seq_chords_rhythm))\n",
    "        seq_lengths = np.asarray(self.mask_lengths[data_index], dtype=np.float32)\n",
    "        seq_output = seq_melody[1:]\n",
    "        \n",
    "        if self.mask_lengths[data_index] < self.sequence_length:\n",
    "            seq_input[self.mask_lengths[data_index], :] = 0.0\n",
    "        \n",
    "        # return original sequence, target sequence, and original sequence lengths\n",
    "        return (\n",
    "            seq_input,\n",
    "            seq_output,\n",
    "            seq_lengths\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim_1, input_dim_2, hidden_dim, batch_size, output_dim, num_layers_bi=2, num_layers_lstm=2, inference=False):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_dim_1 = input_dim_1\n",
    "        self.input_dim_2 = input_dim_2\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers_lstm = num_layers_lstm\n",
    "        self.num_layers_bi = num_layers_bi\n",
    "        self.inference = inference\n",
    "        \n",
    "        self.hidden_bi, self.hidden_lstm = self.init_hidden()\n",
    "        \n",
    "        # Bidirectional LSTM\n",
    "        self.lstm_bi = nn.LSTM(\n",
    "            self.input_dim_1,\n",
    "            self.hidden_dim,\n",
    "            self.num_layers_bi,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # Second pair of LSTM layers\n",
    "        self.lstm = nn.LSTM(self.hidden_dim * 2 + self.input_dim_2, self.hidden_dim, self.num_layers_lstm, batch_first=True)\n",
    "        \n",
    "        # Last dense layer\n",
    "        self.dense = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # This is what we'll initialise our hidden state as\n",
    "        return (\n",
    "            (torch.zeros(self.num_layers_bi * 2, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.num_layers_bi * 2, self.batch_size, self.hidden_dim)),\n",
    "            (torch.zeros(self.num_layers_lstm, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.num_layers_lstm, self.batch_size, self.hidden_dim))\n",
    "               )\n",
    "    \n",
    "    def get_bi_output(self, input_X):\n",
    "        # get chord and rhythm info from input\n",
    "        input_X_1 = input_X[:, :, self.input_dim_2:]\n",
    "        out_bi, _ = self.lstm_bi(input_X_1)\n",
    "        \n",
    "        return out_bi\n",
    "    \n",
    "    def process_lstm_sequence(self, bi_part, melody_part, temperature=1.0, propagate_hidden=False):\n",
    "        concat_X_2 = torch.cat((melody_part, bi_part), 2)\n",
    "            \n",
    "        if propagate_hidden:\n",
    "            out_lstm, self.hidden_lstm = self.lstm(concat_X_2, self.hidden_lstm)\n",
    "        else:\n",
    "            out_lstm, self.hidden_lstm = self.lstm(concat_X_2)\n",
    "        \n",
    "        # Generate sequence predictions\n",
    "        X_pred = self.dense(out_lstm)\n",
    "        \n",
    "        # Apply nonlinearities\n",
    "        X_softmax = F.log_softmax(X_pred / temperature, dim=2)\n",
    "        \n",
    "        return X_softmax\n",
    "\n",
    "    def forward(self, input_X, lengths_X, max_length, temperature=1.0, propagate_hidden=False):\n",
    "        # Forward pass through LSTM layer\n",
    "        # shape of lstm_out: [batch_size, input_size, hidden_dim]\n",
    "        # shape of self.hidden: (a, b), where a and b both have shape (num_layers, batch_size, hidden_dim).\n",
    "        \n",
    "        # get chord and rhythm info from input\n",
    "        input_X_1 = input_X[:, :, self.input_dim_2:]\n",
    "        \n",
    "        if not self.inference:\n",
    "            packed_X_1 = torch.nn.utils.rnn.pack_padded_sequence(input_X_1, lengths_X, batch_first=True)\n",
    "        else:\n",
    "            packed_X_1 = input_X_1\n",
    "            \n",
    "        if propagate_hidden:\n",
    "            out_bi, self.hidden_bi = self.lstm_bi(packed_X_1, self.hidden_bi)\n",
    "        else:\n",
    "            out_bi, self.hidden_bi = self.lstm_bi(packed_X_1)\n",
    "        \n",
    "        if not self.inference:\n",
    "            unpacked_X_1, _ = torch.nn.utils.rnn.pad_packed_sequence(out_bi, batch_first=True, total_length=max_length)\n",
    "        else:\n",
    "            unpacked_X_1 = out_bi\n",
    "        \n",
    "        # Concat output of bi-LSTM with melody part of input\n",
    "        input_X_2 = input_X[:, :, :self.input_dim_2]\n",
    "        concat_X_2 = torch.cat((input_X_2, unpacked_X_1), 2)\n",
    "        \n",
    "        # Feed through second pair of LSTM layers\n",
    "        if not self.inference:\n",
    "            packed_X_2 = torch.nn.utils.rnn.pack_padded_sequence(concat_X_2, lengths_X, batch_first=True)\n",
    "        else:\n",
    "            packed_X_2 = concat_X_2\n",
    "            \n",
    "        if propagate_hidden:\n",
    "            out_lstm, self.hidden_lstm = self.lstm(packed_X_2, self.hidden_lstm)\n",
    "        else:\n",
    "            out_lstm, self.hidden_lstm = self.lstm(packed_X_2)\n",
    "        \n",
    "        if not self.inference:\n",
    "            unpacked_X_2, _ = torch.nn.utils.rnn.pad_packed_sequence(out_lstm, batch_first=True, total_length=max_length)\n",
    "        else:\n",
    "            unpacked_X_2 = out_lstm\n",
    "        \n",
    "        # Generate sequence predictions\n",
    "        X_pred = self.dense(unpacked_X_2)\n",
    "        \n",
    "        # Apply nonlinearities\n",
    "        X_softmax = F.log_softmax(X_pred / temperature, dim=2)\n",
    "        \n",
    "        return X_softmax\n",
    "\n",
    "model = LSTM(input_dim_1=62, input_dim_2=130, hidden_dim=512, batch_size=1, output_dim=62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = MusicDataset('../data/processed_sheets_numpy/data.pkl', 100, data_augmentation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3333 Alan Menken, Stephen Schwartz - Colors Of The Wind.json\n",
      "3391 Alan Menken, Howard Ashman - Skid Row (Downtown).json\n",
      "4710 Alan Menken, Tim Rice - A WHOLE NEW WORLD.json\n",
      "5040 Alan Menken, Howard Ashman - Beauty and the Beast.json\n",
      "5156 Alan Menken, Howard Ashman - Under the Sea.json\n"
     ]
    }
   ],
   "source": [
    "for k, v in md.id_to_sheet.items():\n",
    "    if 'Menken' in v:\n",
    "        print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oakley Haldeman, Gene Autry - Here Comes Santa Claus.json'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md.id_to_sheet[4444]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_loader = DataLoader(md, batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = next(iter(data_train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-4.1127, -4.1692, -4.1670,  ..., -4.1395, -4.0613, -4.1231],\n",
       "         [-4.1121, -4.1686, -4.1671,  ..., -4.1395, -4.0616, -4.1237],\n",
       "         [-4.1104, -4.1684, -4.1670,  ..., -4.1384, -4.0633, -4.1237],\n",
       "         ...,\n",
       "         [-4.1115, -4.1692, -4.1665,  ..., -4.1407, -4.0611, -4.1240],\n",
       "         [-4.1120, -4.1691, -4.1666,  ..., -4.1403, -4.0611, -4.1235],\n",
       "         [-4.1117, -4.1685, -4.1669,  ..., -4.1399, -4.0613, -4.1238]]],\n",
       "       grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(X[0], X[2], 100, propagate_hidden=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (lstm_bi): LSTM(62, 512, num_layers=2, batch_first=True, bidirectional=True)\n",
       "  (lstm): LSTM(1154, 512, num_layers=2, batch_first=True)\n",
       "  (dense): Linear(in_features=512, out_features=130, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LSTM(\n",
    "        input_dim_1=62,\n",
    "        input_dim_2=130,\n",
    "        hidden_dim=512,\n",
    "        batch_size=1,\n",
    "        output_dim=130,\n",
    "        num_layers_bi=2,\n",
    "        num_layers_lstm=2,\n",
    "        inference=True\n",
    "    )\n",
    "\n",
    "model.load_state_dict(torch.load('../chord_rhythm_melody_lstm/test/epoch_60000.model'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('barline', 0, 0)\n",
      "('F', 0, 4)\n",
      "('G', 0, 4)\n",
      "('A', 0, 4)\n",
      "('barline', 0, 0)\n",
      "('A', 1, 4)\n",
      "('C', 0, 5)\n",
      "('D', 0, 5)\n",
      "('barline', 0, 0)\n",
      "('D', 1, 5)\n",
      "('G', 0, 4)\n",
      "('barline', 0, 0)\n",
      "('F', 1, 4)\n",
      "('barline', 0, 0)\n",
      "('D', 0, 5)\n",
      "('F', 0, 4)\n",
      "('barline', 0, 0)\n",
      "('D', 1, 4)\n",
      "('barline', 0, 0)\n",
      "('G', 0, 4)\n",
      "('D', 1, 4)\n",
      "('barline', 0, 0)\n",
      "('D', 0, 4)\n",
      "('barline', 0, 0)\n",
      "('F', 0, 4)\n",
      "('A', 1, 3)\n",
      "('barline', 0, 0)\n",
      "('C', 1, 4)\n",
      "('D', 1, 4)\n",
      "('E', 0, 4)\n",
      "('F', 1, 4)\n",
      "('barline', 0, 0)\n",
      "('G', 1, 4)\n",
      "('D', 1, 5)\n",
      "('barline', 0, 0)\n",
      "('D', 0, 5)\n",
      "('D', 1, 5)\n",
      "('barline', 0, 0)\n",
      "('G', 1, 4)\n",
      "('F', 0, 5)\n",
      "('barline', 0, 0)\n",
      "('D', 1, 5)\n",
      "('C', 1, 5)\n",
      "('barline', 0, 0)\n",
      "('G', 1, 4)\n",
      "('E', 0, 5)\n",
      "('barline', 0, 0)\n",
      "('D', 1, 5)\n",
      "('C', 1, 5)\n",
      "('barline', 0, 0)\n",
      "('F', 1, 4)\n",
      "('G', 1, 4)\n",
      "('A', 1, 4)\n",
      "('barline', 0, 0)\n",
      "('B', 0, 4)\n",
      "('C', 1, 5)\n",
      "('D', 1, 5)\n",
      "('barline', 0, 0)\n",
      "('F', 0, 5)\n",
      "('D', 1, 5)\n",
      "('barline', 0, 0)\n",
      "('D', 0, 5)\n",
      "('C', 0, 5)\n",
      "('barline', 0, 0)\n",
      "('F', 0, 4)\n",
      "('barline', 0, 0)\n",
      "('F', 0, 4)\n",
      "('barline', 0, 0)\n",
      "('F', 0, 4)\n",
      "('barline', 0, 0)\n",
      "('F', 0, 4)\n",
      "('barline', 0, 0)\n",
      "('F', 0, 4)\n",
      "('G', 0, 4)\n",
      "('A', 0, 4)\n",
      "('barline', 0, 0)\n",
      "('A', 1, 4)\n",
      "('C', 0, 5)\n",
      "('D', 0, 5)\n",
      "('barline', 0, 0)\n",
      "('D', 1, 5)\n",
      "('G', 0, 4)\n",
      "('barline', 0, 0)\n",
      "('F', 1, 4)\n",
      "('barline', 0, 0)\n",
      "('D', 0, 5)\n",
      "('F', 0, 4)\n",
      "('barline', 0, 0)\n",
      "('E', 0, 4)\n",
      "('barline', 0, 0)\n",
      "('C', 0, 5)\n",
      "('D', 1, 4)\n",
      "('barline', 0, 0)\n",
      "('D', 0, 4)\n",
      "('barline', 0, 0)\n",
      "('A', 1, 4)\n",
      "('D', 0, 4)\n",
      "('barline', 0, 0)\n",
      "('D', 1, 4)\n",
      "('F', 0, 4)\n",
      "('F', 1, 4)\n",
      "('G', 1, 4)\n",
      "('barline', 0, 0)\n",
      "('A', 1, 4)\n",
      "('F', 1, 5)\n",
      "('barline', 0, 0)\n",
      "('F', 0, 5)\n",
      "('D', 1, 5)\n",
      "('barline', 0, 0)\n",
      "('G', 1, 4)\n",
      "('F', 0, 5)\n",
      "('barline', 0, 0)\n",
      "('D', 1, 5)\n",
      "('C', 1, 5)\n",
      "('barline', 0, 0)\n",
      "('G', 1, 4)\n",
      "('E', 0, 5)\n",
      "('barline', 0, 0)\n",
      "('D', 1, 5)\n",
      "('C', 1, 5)\n",
      "('barline', 0, 0)\n",
      "('F', 1, 4)\n",
      "('G', 1, 4)\n",
      "('A', 1, 4)\n",
      "('barline', 0, 0)\n",
      "('B', 0, 4)\n",
      "('C', 1, 5)\n",
      "('D', 1, 5)\n",
      "('barline', 0, 0)\n",
      "('F', 0, 5)\n",
      "('D', 1, 5)\n",
      "('barline', 0, 0)\n",
      "('D', 0, 5)\n",
      "('C', 0, 5)\n",
      "('barline', 0, 0)\n",
      "('G', 0, 5)\n",
      "('barline', 0, 0)\n",
      "('G', 0, 5)\n",
      "('barline', 0, 0)\n",
      "('G', 0, 5)\n",
      "('F', 0, 5)\n",
      "('barline', 0, 0)\n",
      "('D', 1, 5)\n",
      "('D', 0, 5)\n",
      "('barline', 0, 0)\n",
      "('F', 0, 5)\n",
      "('barline', 0, 0)\n",
      "('F', 0, 5)\n",
      "('barline', 0, 0)\n",
      "('F', 0, 5)\n",
      "('D', 1, 5)\n",
      "('barline', 0, 0)\n",
      "('D', 0, 5)\n",
      "('C', 0, 5)\n",
      "('C', 0, 5)\n",
      "('A', 1, 4)\n",
      "('barline', 0, 0)\n",
      "('A', 1, 4)\n",
      "('barline', 0, 0)\n",
      "('D', 0, 5)\n",
      "('barline', 0, 0)\n",
      "('A', 1, 4)\n",
      "('barline', 0, 0)\n",
      "('C', 0, 5)\n",
      "('F', 0, 5)\n"
     ]
    }
   ],
   "source": [
    "# INFERENCE LOOP\n",
    "model.hidden_bi, model.hidden_lstm = model.init_hidden()\n",
    "X_in = torch.FloatTensor(md.process_sequence(md.data[2322])[0]).unsqueeze(0)\n",
    "bi_output = model.get_bi_output(X_in)\n",
    "\n",
    "model.hidden_bi, model.hidden_lstm = model.init_hidden()\n",
    "\n",
    "lstm_out = model.process_lstm_sequence(bi_output[:, 0:1, :], torch.FloatTensor(md.start_token).unsqueeze(0), temperature=0.9, propagate_hidden=True)\n",
    "next_char = torch.multinomial(torch.exp(lstm_out)[0, 0], 1)\n",
    "print(midi_pitch_to_pitch(int(next_char.detach().numpy())))\n",
    "next_one_hot = torch.FloatTensor(np.zeros((1, 1, 130), dtype=np.float32))\n",
    "next_one_hot[0, 0, next_char] = 1.0\n",
    "\n",
    "for k in range(1, bi_output.shape[1]):\n",
    "    lstm_out = model.process_lstm_sequence(bi_output[:, k:k+1, :], next_one_hot, temperature=0.9, propagate_hidden=True)\n",
    "    next_char = torch.multinomial(torch.exp(lstm_out)[0, 0], 1)\n",
    "    print(midi_pitch_to_pitch(int(next_char.detach().numpy())))\n",
    "    next_one_hot[:, :, :] = 0.0\n",
    "    next_one_hot[0, 0, next_char] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]], dtype=float32)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 1024])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_output[:, 0:1, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 165, 192])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_in.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Constants:\n",
    "    PITCH_CLASSES = [[\"C\", 0], [\"C\", 1], [\"D\", 0], [\"D\", 1], [\"E\", 0], [\"F\", 0], [\"F\", 1], [\"G\", 0], [\"G\", 1], [\"A\", 0], [\"A\", 1], [\"B\", 0]]\n",
    "    \n",
    "def pitch_to_midi_pitch(step, alter, octave):\n",
    "    \"\"\"!@brief Convert MusicXML pitch representation to MIDI pitch number.\n",
    "\n",
    "        @param step Which root note it is (e.g. C, D,...)\n",
    "        @param alter If the pitch was altered (sharp or flat)\n",
    "        @param octave The octave that the pitch is in\n",
    "\n",
    "        @return The MIDI pitch representation of the input\n",
    "    \"\"\"\n",
    "    pitch_class = 0\n",
    "    if step == 'C':\n",
    "        pitch_class = 0\n",
    "    elif step == 'D':\n",
    "        pitch_class = 2\n",
    "    elif step == 'E':\n",
    "        pitch_class = 4\n",
    "    elif step == 'F':\n",
    "        pitch_class = 5\n",
    "    elif step == 'G':\n",
    "        pitch_class = 7\n",
    "    elif step == 'A':\n",
    "        pitch_class = 9\n",
    "    elif step == 'B':\n",
    "        pitch_class = 11\n",
    "    else:\n",
    "        # Raise exception for unknown step (ex: 'Q')\n",
    "        raise Exception('Unable to parse pitch step ' + step)\n",
    "\n",
    "    pitch_class = (pitch_class + int(alter)) % 12\n",
    "    midi_pitch = (12 + pitch_class) + (int(octave) * 12)\n",
    "    return midi_pitch\n",
    "\n",
    "def midi_pitch_to_pitch(midi_pitch):\n",
    "    if midi_pitch == 128:\n",
    "        return ('rest', 0, 0)\n",
    "    elif midi_pitch == 129:\n",
    "        return ('barline', 0, 0)\n",
    "    octave = midi_pitch // 12 - 1\n",
    "    pitch_class = Constants.PITCH_CLASSES[midi_pitch % 12]\n",
    "    step, alter = pitch_class[0], pitch_class[1]\n",
    "\n",
    "    return (step, alter, octave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('rest', 0, 0)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "midi_pitch_to_pitch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
